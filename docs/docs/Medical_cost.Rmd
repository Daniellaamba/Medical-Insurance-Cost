---
title: "<p style=\"color:black,text-align:center\">Medical Insurance Cost</p>"
author: "Daniella Ojekere"
output: h_document
---
# **<span style="color: #16a085;"> SIMPLE LINEAR REGRESSION</span> **

On the target-predictor graph, regression displays a line or curve that traverses each data point in a way that minimizes the vertical distance between the data points and the regression line. A statistical method for simulating the relationship between a dependent variable and a specified collection of independent variables is called linear regression. A linear regression describes the relationship between variables using a straight line. In simple terms, linear regression is a way to understand the relationship between two things (variables). The value of the regression coefficients that minimizes the models overall error determines the line of best fit in the data.
Types of Linear Regression

There are two types of linear regression, Simple linear regression and multiple linear regression. Our discussion in this series will be on simple linear regression using R. 

## **<span style="color: #2C6D26;">Assumptions of Simple Linear Regression</span> ** 

The assumptions of simple linear regressions are:

1.	**L**inearity: Ensure there is a linear relationship between the independent (X) and dependent variable(Y), a straight line represents the line that best fits the data points. 

2.	**I**ndependence: The independent and dependent variables have a linear relationship. This indicates that when one variable changes, the other variable changes proportionately.

3.	**N**ormality: The errors are normally distributed

4.	**E**quality of variance: The variability of the response does not increase as the value of the predictor increases.
Case Study

**LINE** is a simple acroynm for remembering the assumptions of simple linear regression.

## **<span style="color: #2C6D26;">Example</span> ** 

In this example, we will be using a dataset on medical insurance cost. The dataset consists of information about the insurance buyers such as age, sex, BMI (body mass index), number of children, smoking habits and region. These variables serve as independent features, while the medical charges represent the dependent feature. The goal is to predict the medical charges based on the age of the individual.

### **<span style="color: #138d75;">Install packages</span> ** 
```{r setup, warning=FALSE, message=FALSE}
install.packages("tidyverse")
install.packages("readxl")

```

### **<span style="color: #138d75;">Load the installed packages</span> ** 
```{r, warning = FALSE, message = FALSE}
library(tidyverse)
library(readxl)
```

### **<span style="color: #138d75;">Load the datasets</span> ** 
```{r, warning = FALSE, message = FALSE}
medical_cost <- read_csv("insurance.csv")
```

## **<span style="color: #2C6D26;">Data Exploration</span> ** 

### **<span style="color: #138d75;"> List structure of the dataset</span> ** 
```{r}
str(medical_cost)
```
### **<span style="color: #138d75;"> Summary of the dataset</span> ** 
```{r}
summary(medical_cost)
```

### **<span style="color: #138d75;"> Check the names in the dataset</span> ** 
```{r}
names(medical_cost)
```
### **<span style="color: #138d75;">  Check the number of rows</span> **

```{r}
nrow(medical_cost)
```

### **<span style="color: #138d75;"> Check number of columns</span> ** 
```{r}
ncol(medical_cost)
```


### **<span style="color: #138d75;"> See first 6 rows of the dataset</span> ** 
```{r}
head(medical_cost)
```

### **<span style="color: #138d75;"> Check the last 6 rows of the dataset</span> **  
```{r}
tail(medical_cost)
```
### **<span style="color: #138d75;">  Rename sex variable to gender</span> ** 
```{r}
medical_cost <- medical_cost |> 
  mutate(across(c(sex: region ), as.factor)) |> 
  rename(gender = sex)
```


### **<span style="color: #138d75;">  Check for missing values</span> ** 
```{r}
sum(is.na(medical_cost))
```

### **<span style="color: #138d75;"> Check for duplicates</span> **  
```{r}
sum(duplicated(medical_cost))
```

There is one duplicate, so we drop the duplicate value and work with the unique values

```{r}
#Remove duplicates    
medical_cost <- medical_cost[!duplicated(medical_cost),]
```
The dataset changes from 1338 to 1337 because the duplicate value has been removed. 

Since this is a simple linear regression, I will be checking how the age of a patient affects the hospital charges. 


### **<span style="color: #138d75;"> Visualizing the distribution of the age  variable</span> **  
```{r}

age_plot <- ggplot(medical_cost, aes(x = age)) +
  geom_histogram(fill = "steelblue", color = "black", bins = 10) +
  labs(title = "Distribution of Age", x = "Age", y = "Frequency")

interactive_plot <- plotly::ggplotly(age_plot)
interactive_plot
```



## **<span style="color:  #2C6D26;"> Checking if it meets up the Assumptions</span> ** 
### **<span style="color: #138d75;">  1. Linearity</span> ** 

```{r}
plot(charges ~ age, data = medical_cost)
```
This shows the linearity of age against charges. 



### **<span style="color: #138d75;"> 2. Independence </span> ** 
```{r}

cor(medical_cost$age, medical_cost$charges)


```

The correlation between age and charges is 0.3 which is not close to 1 or -1. This shows that the variables are independent of each other.



### **<span style="color: #138d75;"> Fitting the regression model </span> ** 



```{r}
model <- lm(charges ~ age, data = medical_cost)
model
```
```{r}
summary(model)
```
```{r}
plot(model)
```

The Q-Q residuals shows non-normality of the residuals.Hence, the dependent variable will be transformed to log to see if it will improve the normality of the residuals.

```{r}
model2 <- lm(log(charges) ~ age, data = medical_cost)
model2
```
```{r}
summary(model2)
```
The estimated effect of age on charges is 0.034. This means that for every one unit increase in age, the charges increases by 0.034. The p-value is less than 0.05 which means that the effect of age on charges is statistically significant. The R-squared value is 0.09 which means that 9% of the variation in charges can be explained by the age of the patient.
The standard error of the estimate is 0.1, with t value of 22.65  which means that the model is a good fit.

```{r}
plot(model2)
```
The Residual vs Fitted shows that the residuals are homoscedastic and linear, and the Q-Q residuals shows that the residuals are normally distributed.

Therefore, fitting the regression model with the log of charges is a better fit. The regression equation is given as:
Charges = 0.034*age + 7.5
