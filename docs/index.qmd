---
title: "<p style=\"color:black,text-align:center\">Simple Linear Regresson</p>"
author: "Daniella Ojekere"
format:
  html:
    toc-location: right
    toc: true
    toc-title: Outline
    toc-depth: 3
    toc_float:
    collapsed: True
    smooth_scroll: true
    code_folding: show
---

### 

# [**SIMPLE LINEAR REGRESSION**]{style="color: #16a085;"}

On the target-predictor graph, regression displays a line or curve that traverses each data point in a way that minimizes the vertical distance between the data points and the regression line. A statistical method for simulating the relationship between a dependent variable and a specified collection of independent variables is called linear regression. A linear regression describes the relationship between variables using a straight line. In simple terms, linear regression is a way to understand the relationship between two things (variables). The value of the regression coefficients that minimizes the models overall error determines the line of best fit in the data. Types of Linear Regression

There are two types of linear regression, Simple linear regression and multiple linear regression. Our discussion in this series will be on simple linear regression using R.

## [**Assumptions of Simple Linear Regression**]{style="color: #2C6D26;"}

The assumptions of simple linear regressions are:

1.  **L**inearity: Ensure there is a linear relationship between the independent (X) and dependent variable(Y), a straight line represents the line that best fits the data points.

2.  **I**ndependence: The independent and dependent variables have a linear relationship. This indicates that when one variable changes, the other variable changes proportionately.

3.  **N**ormality: The errors are normally distributed

4.  **E**quality of variance: The variability of the response does not increase as the value of the predictor increases. Case Study

**LINE** is a simple acroynm for remembering the assumptions of simple linear regression.

## [**Example**]{style="color: #2C6D26;"}

In this example, we will be using a dataset on medical insurance cost. The dataset consists of information about the insurance buyers such as age, sex, BMI (body mass index), number of children, smoking habits and region. These variables serve as independent features, while the medical charges represent the dependent feature. The goal is to predict the medical charges based on the age of the individual.

### [**Install packages**]{style="color: #138d75;"}

```{r setup, warning=FALSE, message=FALSE}
install.packages("tidyverse") 
install.packages("readxl") 
```

### [**Load the installed packages**]{style="color: #138d75;"}

```{r, warning = FALSE, message = FALSE}
library(tidyverse) 
library(readxl)
```

### [**Load the datasets**]{style="color: #138d75;"}

```{r, warning = FALSE, message = FALSE}
medical_cost <- read_csv("insurance.csv")
```

## [**Data Exploration**]{style="color: #2C6D26;"}

### [**List structure of the dataset**]{style="color: #138d75;"}

```{r}
str(medical_cost)
```

### [**Summary of the dataset**]{style="color: #138d75;"}

```{r}
summary(medical_cost)
```

### [**Check the names in the dataset**]{style="color: #138d75;"}

```{r}
names(medical_cost)
```

### [**Check the number of rows**]{style="color: #138d75;"}

```{r}
nrow(medical_cost)
```

### [**Check number of columns**]{style="color: #138d75;"}

```{r}
ncol(medical_cost)
```

### [**See first 6 rows of the dataset**]{style="color: #138d75;"}

```{r}
head(medical_cost)
```

### [**Check the last 6 rows of the dataset**]{style="color: #138d75;"}

```{r}
tail(medical_cost)
```

### [**Rename sex variable to gender**]{style="color: #138d75;"}

```{r}
medical_cost <- medical_cost |>   
  mutate(across(c(sex: region ), as.factor)) |>   
  rename(gender = sex)
```

### [**Check for missing values**]{style="color: #138d75;"}

```{r}
sum(is.na(medical_cost))
```

### [**Check for duplicates**]{style="color: #138d75;"}

```{r}
sum(duplicated(medical_cost))
```

There is one duplicate, so we drop the duplicate value and work with the unique values

`{r} #Remove duplicates} medical_cost <- medical_cost[!duplicated(medical_cost),]`

The dataset changes from 1338 to 1337 because the duplicate value has been removed.

Since this is a simple linear regression, I will be checking how the age of a patient affects the hospital charges.

### [**Visualizing the distribution of the age variable**]{style="color: #138d75;"}

```{r}
age_plot <- ggplot(medical_cost, aes(x = age)) +  
  geom_histogram(fill = "steelblue", color = "black", bins = 10) +  
  labs(title = "Distribution of Age", x = "Age", y = "Frequency")  

interactive_plot <- plotly::ggplotly(age_plot) 
interactive_plot
```

## [**Checking if it meets up the Assumptions**]{style="color:  #2C6D26;"}

### [**1. Linearity**]{style="color: #138d75;"}

```{r}
plot(charges ~ age, data = medical_cost)
```

This shows the linearity of age against charges.

### [**2. Independence**]{style="color: #138d75;"}

```{r}
cor(medical_cost$age, medical_cost$charges)  
```

The correlation between age and charges is 0.3 which is not close to 1 or -1. This shows that the variables are independent of each other.

**3. Normality and Homoscedasticity will be tested after fitting the model.**

### [**Fitting the regression model**]{style="color: #138d75;"}

```{r}
model <- lm(charges ~ age, data = medical_cost)
model
```

```{r}
summary(model)
```

```{r}
plot(model)
```

The Q-Q residuals shows non-normality of the residuals.Hence, the dependent variable will be transformed to log to see if it will improve the normality of the residuals.

```{r}
model2 <- lm(log(charges) ~ age, data = medical_cost) 
model2
```

```{r}
summary(model2)
```

The estimated effect of age on charges is 0.034. This means that for every one unit increase in age, the charges increases by 0.034. The p-value is less than 0.05 which means that the effect of age on charges is statistically significant. The R-squared value is 0.09 which means that 9% of the variation in charges can be explained by age. The standard error of the estimate is 0.1, with t value of 22.65 which means that the model is a good fit.

```{r}
plot(model2)
```

The Residual vs Fitted shows that the residuals are homoscedastic and linear, and the Q-Q residuals shows that the residuals are normally distributed.

Therefore, fitting the regression model with the log of charges is a better fit. The regression equation is given as: Charges = 0.034\*age + 7.5
